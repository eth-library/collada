{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raq-wJ9pNggc"
      },
      "source": [
        "# Topic Modeling\n",
        "In this notebook we create a topic model for the text and also several topic models per article. The topic models are created as word clouds for easy visualization but tables are also possible.\n",
        "\n",
        "Topic modeling overall depends on your usecase and what you would like to find out about your data, but I would not reccommend it for similarity search. For that usecase vector embeddings do a better, more reliable job. If you would like to get an idea of what the documents are about without having to read them entirely, or even if you would like to classify many documents without knowing beforehand what the classes might be, topic modeling is a good option. Though these days, even for that usecase embedding the documents with a trained word or document embedding model such as SBERT and then clustering said embeddings does a better job of splitting documents into classes. Another advantage of clustering embeddings is that you do not need to know the number of topics in advance\n",
        "\n",
        "Still, many scientific articles in the humanities use topic modeling, so this introduction aims to show a minimal topic modeling example, which you can yourself play with and verify.\n",
        "\n",
        "First let us import the necessary libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA6ok_Z0Nggd"
      },
      "outputs": [],
      "source": [
        "# packages to store and manipulate data\n",
        "import pandas as pd\n",
        "# plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "import spacy\n",
        "from tqdm import tqdm as tq\n",
        "from random import shuffle\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "from numpy.random import randint\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxV_xXNYNggd"
      },
      "source": [
        "Then load the text and create a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stHoVIKDNggd"
      },
      "outputs": [],
      "source": [
        "pages = [\"grs-001_1921_13__298_d.txt\", \"grs-001_1921_13__393_d.txt\", \"grs-001_1922_14__563_d.txt\", \"grs-001_1923_15__447_d.txt\"]\n",
        "input_text = []\n",
        "for page in pages:\n",
        "    with open(\"data/embedding_data/\"+page,\"r\") as fin:\n",
        "        input_text.append(fin.readlines())\n",
        "df = pd.DataFrame(input_text, columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have a look at the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_stopwords = nltk.corpus.stopwords.words('german')\n",
        "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
        "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@»«—°”’“'\n",
        "\n",
        "# cleaning master function\n",
        "def clean_text(text, bigrams=False):\n",
        "    text = text.lower() # lower case\n",
        "    text = re.sub('['+my_punctuation + ']+', ' ', text) # strip punctuation\n",
        "    text = re.sub('\\s+', ' ', text) #remove double spacing\n",
        "    text = re.sub('([0-9]+)', '', text) # remove numbers\n",
        "    text_token_list = [word for word in text.split(' ')\n",
        "                            if word not in my_stopwords] # remove stopwords\n",
        "\n",
        "    text_token_list = [word_rooter(word) if '#' not in word else word\n",
        "                        for word in text_token_list] # apply word rooter\n",
        "    if bigrams:\n",
        "        text_token_list = text_token_list+[text_token_list[i]+'_'+text_token_list[i+1]\n",
        "                                            for i in range(len(text_token_list)-1)]\n",
        "    text = ' '.join(text_token_list)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['text'] = df.text.apply(clean_text) #this takes around 1m20s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHhaFjSnNgge"
      },
      "source": [
        "## Entire text\n",
        "This is topic modeling on the level of the entire text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyHmn2PkNggg"
      },
      "source": [
        "### WordCloud\n",
        "In this type of topic modeling, we don't do much by hand but can display the topics in a nice \"word cloud\". This can be used for visualizations in reports or presentations.\n",
        "\n",
        "But first, once more, we decide on the number of topics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXS8lv49Nggg"
      },
      "outputs": [],
      "source": [
        "N_TOPICS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib-2X9ytNggh",
        "outputId": "527e74c0-096b-499f-f18c-dd86d758502f"
      },
      "outputs": [],
      "source": [
        "#nlp = spacy.load('en_core_web_sm') # for english text\n",
        "nlp = spacy.load('de_core_news_sm')\n",
        "\n",
        "def tokenize(x, nlp):\n",
        "    # lemmatize and lowercase without stopwords, punctuation and numbers\n",
        "    return [w.lemma_.lower() for w in nlp(x) \n",
        "            if not w.is_stop and not w.is_punct and not w.is_digit and len(w) > 2]\n",
        "\n",
        "# split into paragraphs\n",
        "doc_clean = []\n",
        "for doc in tq(df['text']):\n",
        "    # split by paragraph\n",
        "    for paragraph in doc.split(\"\\n\\n\"):\n",
        "        doc_clean.append(tokenize(paragraph, nlp))\n",
        "\n",
        "# randomize document order\n",
        "shuffle(doc_clean)\n",
        "\n",
        "# creating the term dictionary\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "# filter extremes, drop all words appearing in less than 10 paragraphs and all words appearing in at least every third paragraph\n",
        "# dictionary.filter_extremes(no_below=10, no_above=0.33, keep_n=1000)\n",
        "\n",
        "# creating the document-term matrix\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "# train LDA with 10 topics and print\n",
        "lda = LdaModel(doc_term_matrix, num_topics=N_TOPICS,\n",
        "               id2word = dictionary, passes=3)\n",
        "lda.show_topics(formatted=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDaXgJxNNggh"
      },
      "source": [
        "With the computations done, let us look at the nice word clouds :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BDZT108bNggh",
        "outputId": "c797fdb3-7fa3-4b23-eaf0-ffe44505e7ef"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# LDA Word Clouds\n",
        "###\n",
        "#https://github.com/elliottash/nlp_lss_2023/blob/master/notebooks/03_topic_models.ipynb\n",
        "\n",
        "# make word clouds for the topics\n",
        "for i,weights in lda.show_topics(num_topics=N_TOPICS,\n",
        "                                 num_words=100,\n",
        "                                 formatted=False):\n",
        "\n",
        "    maincol = randint(0,360)\n",
        "    def colorfunc(word=None, font_size=None,\n",
        "                  position=None, orientation=None,\n",
        "                  font_path=None, random_state=None):\n",
        "        color = randint(maincol-10, maincol+10)\n",
        "        if color < 0:\n",
        "            color = 360 + color\n",
        "        return \"hsl(%d, %d%%, %d%%)\" % (color,randint(65, 75)+font_size / 7, randint(35, 45)-font_size / 10)\n",
        "\n",
        "\n",
        "    wordcloud = WordCloud(background_color=\"white\",\n",
        "                          ranks_only=True,\n",
        "                          max_font_size=120,\n",
        "                          color_func=colorfunc,\n",
        "                          height=600,width=800).generate_from_frequencies(dict(weights))\n",
        "\n",
        "    plt.clf()\n",
        "    plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \"Manual\" Topic Modeling\n",
        "Here instead of using spacy we create everything ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we count the words. With this count vectorized we can get words that are mentioned in at most 90% of the documents with the parameter max_df=0.9 so as to avoid filler words. We can also ignore words that are mentioned in less than x% of documents using \"min_df=x\". But since we work with merely four documents, we will simply used our pre-cleaned text and not reduce it further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the vectorizer object will be used to transform text to vector form\n",
        "vectorizer = CountVectorizer(token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
        "\n",
        "# apply transformation\n",
        "tf = vectorizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# tf_feature_names tells us what word each column in the matric represents\n",
        "tf_feature_names = vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can decide on how many topics we would like to extract and then call the model. In this case le'ts choose 4 topics, one per document, and see if the topic model can properly differentiate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_topics = 4\n",
        "\n",
        "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
        "\n",
        "model.fit(tf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a \"helper function\", it makes sure the result is printed in a nice way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    topic_dict = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "    return pd.DataFrame(topic_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now all we have to do is decide how many top words we would like to see per topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_top_words = 10\n",
        "display_topics(model, tf_feature_names, no_top_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The topics themselves don't have a title except for \"topic k\" since this is done in a mathematical way where similar words are clustered together and not necessarily in a way where the model finds n topics and then splits the rest.\n",
        "\n",
        "It seems that the topic modeling did naturally split the documents into four topics, but let's confirm by creating the topic models per document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Per Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(1,len(df)+1):\n",
        "    # apply transformation\n",
        "    tf_i = vectorizer.fit_transform(df['text'][i-1:i]).toarray()\n",
        "\n",
        "    # tf_feature_names tells us what word each column in the matric represents\n",
        "    tf_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    number_of_topics = 1\n",
        "\n",
        "    model_i = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
        "\n",
        "    model_i.fit(tf_i)\n",
        "\n",
        "    no_top_words = 10\n",
        "    print(display_topics(model_i, tf_feature_names, no_top_words))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indeed! Although we concatenated the articles before, even then our topic model was able to discern that they were different articles with different topics."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GHhaFjSnNgge"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
