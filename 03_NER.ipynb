{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER\n",
    "Named entity recognition is the cornerstone for many applications. From automatically [building a database of herbs](https://www.nature.com/articles/s41598-023-50179-0#additional-information) to [end-to-end Named Entity Linking](https://zenodo.org/records/13907910), it all begins with simply recognizing what entities are mentioned in text.\n",
    "\n",
    "For German text, the two frameworks we'll introduce are [SpaCy](https://spacy.io/models) and [FlairNLP](https://github.com/flairNLP/flair?tab=readme-ov-file). We show each step for both and encourage you to test out both on your datasets to see which one performs better in your particular use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import string\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import re\n",
    "import requests\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from xml.etree.ElementTree import Element, SubElement, ElementTree\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you have to comment in the part of the code that describes your situation.\n",
    "\n",
    "The first part is to load the PDF from the E-Periodica website without actually downloading it onto your machine, the second part is for the case where you already have the PDF on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the text\n",
    "Below we offer the code to load several input types (.pdf, .txt, .xml, and image files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF input\n",
    "If you use PDFs from a Website or from your computer, run these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to save the PDF locally, comment in the next four lines and comment out the last one.\n",
    "url = \"https://www.e-periodica.ch/cntmng?pid=grs-001%3A1921%3A13%3A%3A298\"\n",
    "r = requests.get(url)\n",
    "f = io.BytesIO(r.content)\n",
    "parsed = parser.from_buffer(f)\n",
    "\n",
    "# If you've downloaded the PDF onto your computer, comment in the following line and comment out the previous four:\n",
    "#parsed = parser.from_file('data/ner_data/grs-001_1921_13__298_d.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The E-Periodica PDFs already have the OCR embedded into them, so all you need to do is extract the text and clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf\n",
    "contents = [x.strip() for x in parsed[\"content\"].split(\"\\n\") if x != \"\"]\n",
    "#remove the first page\n",
    "article = contents[contents.index('https://www.e-periodica.ch/digbib/terms?lang=en')+1:]  # remove the first page of metadata\n",
    "article = \"\\n\".join(article)  # I deliberately add newlines so we can nicely put words back together that were split across the pages\n",
    "\n",
    "article = re.sub(\"¬\\n\", \"\", article)  # \"bindestriche\" will be removed, if they are followed by one or several whitespaces, those will be removed as well.\n",
    "article = article.strip()  # remove all starting and trailing whitespaces\n",
    "article = re.sub(\"\\n\", \" \", article)  # replace newlines with spaces\n",
    "article = re.sub(r'\\s+', \" \", article)  # replace all repeating whitespaces with only one whitespace\n",
    "article = re.sub(r'\\\\', \"\", article)  # replace all double backslashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly different from how we read in the embedded text from the PDF in 01_text_recognition. There is really no difference, but doing it via requests with a parser makes it more general so you can choose between loading it from your computer or from the internet and then work with the resulting file the exact same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Output\n",
    "If you would like to now save this PDF text as a simple text file, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"data/ner_data/output/grs-001_1921_13__298_d.txt\"\n",
    "with open(output_filename,\"w\") as f:\n",
    "    f.writelines(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Input\n",
    "If you have the input as a text file, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text\n",
    "input_filepath = \"data/ner_data/output/grs-001_1921_13__298_d.txt\"\n",
    "with open(input_filepath, \"r\") as f:\n",
    "    article = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Input\n",
    "If you have the input as an XML file, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml\n",
    "input_filepath = \"./data/ocr_data/grs-001_1921_13__298_d_tei.xml\"\n",
    "with open(input_filepath, \"r\") as f:\n",
    "    article = f.read()\n",
    "soup = BeautifulSoup(article, features=\"xml\")\n",
    "pageText = soup.findAll(text=True)\n",
    "article = \" \".join(pageText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Input\n",
    "If you have the input as an image file only, please check out the 01_text_recognition notebook, save the text files and run the cells for the Text input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair\n",
    "First, we'll show you how named entity tagging works with FlairNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tagger, this might take a while but german-large has much better performance than the smaller models\n",
    "tagger = SequenceTagger.load(\"flair/ner-german-large\")\n",
    "\n",
    "# predict on the article you loaded above.\n",
    "sentence = Sentence(article)\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract certain tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = []\n",
    "places = []\n",
    "organisations = []\n",
    "\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    if entity.tag == \"PER\": #people\n",
    "        name = entity.text.translate(str.maketrans('', '', string.punctuation)) #remove possible ocr mistakes\n",
    "        if len(name) >= 3: # names are usually not shorter\n",
    "            people.append(entity.text)\n",
    "    elif entity.tag == \"LOC\": #places\n",
    "        place = entity.text.translate(str.maketrans('', '', string.punctuation)) #remove possible ocr mistakes\n",
    "        if len(place) >= 3: # place are usually not shorter\n",
    "            places.append(entity.text)\n",
    "    elif entity.tag == \"ORG\": #only organisations\n",
    "        org = entity.text.translate(str.maketrans('', '', string.punctuation)) #remove possible ocr mistakes\n",
    "        if len(org) >= 3:\n",
    "            organisations.append(entity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we chose an article with no people mentioned. This can happen frequently, especially if the article is about international relations or laws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is only People / Places / Organizations. What if you also want to extract dates and numbers (cardinal) and times? For that you need the [ontonotes](https://catalog.ldc.upenn.edu/LDC2013T19) model. FlairNLP only has this model trained on English content, but as you'll see, it will work surprisingly well even on German text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_onto = SequenceTagger.load(\"flair/ner-english-ontonotes-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_test = Sentence(\"Dem will eine Datenschutztagung an der ETH Zürich dienen, die von einer Gruppe Gewerkschaftern organisiert und vom SGB und seinen Verbänden unterstützt wird. Sie findet Samstag, den 31. März 1984 ab 9.15 Uhr, ganztägig statt.\")\n",
    "tagger_onto.predict(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_test.get_spans('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "Now we'll do the same thing, but with SpaCy. The results for our toy examples are identical between SpaCy and FlairNLP, but keep in mind that SpaCy is significantly faster when you have much more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### German NER\n",
    "First, just as with FlairNLP, we show the results for regular German entity tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = spacy.load(\"de_core_news_lg\")\n",
    "doc_de = nlp_de(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What tags are even possible for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de.get_pipe('ner').labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike FlairNLP though, SpaCy has a really nice visualization capability. (Don't forget to stop the execution of the cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.serve(doc_de, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find named entities, phrases and concepts\n",
    "for entity in doc_de.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we have some text with lots of time and dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_2 = nlp_de(\"Dem will eine Datenschutztagung an der ETH Zürich dienen, die von einer Gruppe Gewerkschaftern organisiert und vom SGB und seinen Verbänden unterstützt wird. Sie findet Samstag, den 31. März 1984 ab 9.15 Uhr, ganztägig statt.\")\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc_2.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Onto NER\n",
    "In that case we need to step it up. Using a model trained on the ontonotes tags once again, but only the English ones, we get many more labels to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.get_pipe('ner').labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize it! SpaCy has a built-in visualizer, but careful! Since we're using a Jupyter notebook, the \"serve\" function will not stop on its own. You have to \"interrupt\" the display to run the next cells. There is a function specifically for Jupyter notebooks, namely \"displacy.render\", but that one does not display properly in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But does it work on our German text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_2 = nlp(\"Dem will eine Datenschutztagung an der ETH Zürich dienen, die von einer Gruppe Gewerkschaftern organisiert und vom SGB und seinen Verbänden unterstützt wird. Sie findet Samstag, den 31. März 1984 ab 9.15 Uhr, ganztägig statt.\")\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc_2.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sure does! Looks great, despite the fact that this model was only trained on English data. That implies that there's some kind of generality to the rules it is learning for entity tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to TEI XML with SpaCy NER\n",
    "Digital humanities mostly work with XML files and NER lends itself to be incorporated in a typical XML file structure. Here we take a text file, run NER on it and save it as a TEI XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_path = \"data/embedding_data/grs-001_1921_13__298_d.txt\"\n",
    "output_file_path = \"data/ocr_data/output/grs-001_1921_13__298_d_tei.xml\"\n",
    "output_file_ner = \"data/ocr_data/output/grs-001_1921_13__298_d_tei_ner.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we save the .txt file into a TEI XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tei_from_txt(txt_file_path, output_file_path, paragraph_delimiter=\"\\n\", page_delimiter=\"\\n\\n\",):\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    pages = text.split(page_delimiter)\n",
    "    paragraphs = [x.split(paragraph_delimiter) for x in pages]\n",
    "\n",
    "    tei = Element('teiHeader') #root\n",
    "    text_section = SubElement(tei, 'text')\n",
    "    body = SubElement(text_section, 'body')\n",
    "    \n",
    "    for page in paragraphs:\n",
    "        p_page = SubElement(body,\"pb\")\n",
    "        for paragraph in page:\n",
    "            p_para = SubElement(p_page, 'p')  # Paragraph element\n",
    "            p_para.text = paragraph\n",
    "    \n",
    "    # Generate the output XML file\n",
    "    tree = ElementTree(tei)\n",
    "    tree.write(output_file_path, encoding='utf-8', xml_declaration=True)\n",
    "    \n",
    "    print(f\"TEI file created: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tei_from_txt(txt_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run NER on said file, and save it with the named entities tagged in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_tei_from_tei(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        xml_doc = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(xml_doc, \"xml\")\n",
    "\n",
    "    paragraphs = soup.find_all(string=True)\n",
    "    for entry in paragraphs:\n",
    "\n",
    "        doc = nlp(entry.text) #change here for flair\n",
    "        newtext = entry\n",
    "        last_tag = \"\"\n",
    "        running_total = 0\n",
    "        for i,ent in enumerate(doc.ents): #change the enumeration of the entities for flair\n",
    "            start = ent.start_char + running_total\n",
    "            end = ent.end_char + running_total\n",
    "            entity_text = ent.text\n",
    "            entity_label = ent.label_\n",
    "\n",
    "            if entity_label == \"PER\": #change the tags for flair\n",
    "                tag = \"perName\"\n",
    "            elif entity_label == \"ORG\":\n",
    "                tag = \"orgName\"\n",
    "            elif entity_label == \"GPE\" or entity_label == \"LOC\":\n",
    "                tag = \"placeName\"\n",
    "            else:\n",
    "                tag = entity_label\n",
    "            \n",
    "            newtext = newtext[:start] + \"<\"+tag+\">\"+entity_text+\"</\"+tag+\">\" + newtext[end:]\n",
    "            last_tag = tag\n",
    "            running_total += (5+2*len(last_tag))\n",
    "        \n",
    "        entry.replace_with(BeautifulSoup(newtext, features=\"html.parser\"))\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ner_tei_from_tei(output_file_path, output_file_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know the results look somewhat disappointing here, for instance \"Zwangsmassregeln\" being tagged as a person is less than ideal...\n",
    "\n",
    "Picking the proper model for your dataset (or finetuning / training it yourself) is the biggest part of this process, everything else can be done automatically. Here, I simply chose SpaCy's biggest German model, which was trained on news stories. A FlairNLP model might do better, or a model trained on books even."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_datastories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
