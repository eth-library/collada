{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuPYHiwzNggj"
      },
      "source": [
        "# Embedding space\n",
        "Here we embed the articles in order to be able to find similar ones simply based on semantic content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/genta/Documents/notebooks_cs/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uqnCcJ85wMu"
      },
      "source": [
        "Here you create the articles dictionary based on txt files. If you only have PDFs, check out the notebook on OCR which explains not only how to apply OCR on images but also how to extract embedded OCR text from PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OwgJR9CW5vr0"
      },
      "outputs": [],
      "source": [
        "with open(\"data/embedding_data/grs-002_1984_76__216_d.txt\",\"r\") as fin:\n",
        "    input_text_1 = fin.readlines()\n",
        "    if len(input_text_1) == 1:\n",
        "        input_text_1 = input_text_1[0].split(\".\")\n",
        "\n",
        "with open(\"data/embedding_data/grs-002_1984_76__218_d.txt\",\"r\") as fin:\n",
        "    input_text_2 = fin.readlines()\n",
        "    if len(input_text_2) == 1:\n",
        "        input_text_2 = input_text_2[0].split(\".\")\n",
        "\n",
        "with open(\"data/embedding_data/grs-002_1984_76__219_d.txt\",\"r\") as fin:\n",
        "    input_text_4 = fin.readlines()\n",
        "    if len(input_text_4) == 1:\n",
        "        input_text_4 = input_text_4[0].split(\".\")\n",
        "\n",
        "with open(\"data/embedding_data/grs-002_1984_76__277_d.txt\",\"r\") as fin:\n",
        "    input_text_3 = fin.readlines()\n",
        "    if len(input_text_3) == 1:\n",
        "        input_text_3 = input_text_3[0].split(\".\")\n",
        "\n",
        "#create dictionary for each article\n",
        "article_dict = {\"article1\": {\"text\": input_text_1},\n",
        "                \"article2\": {\"text\": input_text_2},\n",
        "                \"article3\": {\"text\": input_text_3},\n",
        "                \"article4\": {\"text\": input_text_4}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT\n",
        "This part is more for your understanding than anything else. BERT is not ideal for embedding large amounts of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrvrB4Od6RjA"
      },
      "source": [
        "\n",
        "Here we load the necessary libraries, tokenizer and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JFkkAPHNggj"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
        "model = BertModel.from_pretrained('bert-base-german-cased',\n",
        "                                  output_hidden_states = True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW_dpTvk6f92"
      },
      "source": [
        "This part checks if we have a GPU, and if we do it runs the computation on GPU. That makes the embedding much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuTHGLSm6fBB"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx4gvWpdNggj"
      },
      "source": [
        "We go through each article and embed it. To be exact, we embed each word and then average it out over a \n",
        "sentence. For four articles this takes 1 minute on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC2jAh2QNggj"
      },
      "outputs": [],
      "source": [
        "for article_i in article_dict:\n",
        "    bert_emb = []\n",
        "    for sentence in article_dict[article_i][\"text\"]:\n",
        "        bert_tokens = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(bert_tokens[\"input_ids\"], bert_tokens[\"attention_mask\"])\n",
        "            state = outputs[\"last_hidden_state\"]\n",
        "            emb = (state*bert_tokens[\"attention_mask\"][:,:,None]).sum(dim=1) / bert_tokens[\"attention_mask\"][:,:,None].sum(dim=1)\n",
        "            bert_emb.append(emb)\n",
        "    article_dict[article_i][\"bert_embedding\"] = bert_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5-oE2KVNggk"
      },
      "source": [
        "We average everything each article says and create a matrix of the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bmQXDlC5Nggk"
      },
      "outputs": [],
      "source": [
        "V = []\n",
        "origin = np.array([[0]*768,[0]*768])\n",
        "for article_i in article_dict.keys():\n",
        "    mean = (sum(article_dict[article_i][\"bert_embedding\"]) / len(article_dict[article_i][\"bert_embedding\"]))\n",
        "    V.append(mean)\n",
        "V = torch.stack(V).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZw76sxDNggk"
      },
      "source": [
        "We create the distance matrix of each article to each article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PUe2x8tJNggk"
      },
      "outputs": [],
      "source": [
        "distance_matrix = metrics.pairwise_distances(V[:,0,:])\n",
        "\n",
        "#the distance of the article_i to itself is set to infinite\n",
        "for i in range(len(distance_matrix)):\n",
        "    distance_matrix[i][i] = np.inf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "distance_matrix = distance_matrix.round(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   inf, 2.6734, 2.8058, 2.3782],\n",
              "       [2.6734,    inf, 3.0038, 2.4659],\n",
              "       [2.8058, 3.0038,    inf, 2.7493],\n",
              "       [2.3782, 2.4659, 2.7493,    inf]], dtype=float32)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "distance_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the distances are okay-ish but it doesn't really show how similar article1 and article4 are, it just looks like they're slightly more similar than article4 and the other two articles. Can we do better? Yes. So far we've averaged the embeddings twice, once over each sentence and then even over each article! A lot of information gets lost there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SentenceTransformer\n",
        "The [SentenceTransformer](https://sbert.net/) is a fine-tuned version of BERT, where the authors of the paper trained a so-called siamese network in order to have a model specifically trained for similarity. Recall that BERT does similarity moreso out of the definition of a vector, rather than on purpose. \n",
        "\n",
        "Siamese networks train the vector embeddings using a positive and a negative example, and then try to jointly maximize the distance from our vector to the negative example, as well as minimize the distance from our vector to the positive example. Let's see how they do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "#https://sbert.net/docs/sentence_transformer/pretrained_models.html#semantic-similarity-models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \" \".join(article_dict[\"article1\"][\"text\"]),\n",
        "    \" \".join(article_dict[\"article2\"][\"text\"]),\n",
        "    \" \".join(article_dict[\"article3\"][\"text\"]),\n",
        "    \" \".join(article_dict[\"article4\"][\"text\"])\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "distiluse-base-multilingual-cased-v1\n",
            "tensor([[1.0000, 0.3065, 0.1706, 0.5132],\n",
            "        [0.3065, 1.0000, 0.1807, 0.3198],\n",
            "        [0.1706, 0.1807, 1.0000, 0.2846],\n",
            "        [0.5132, 0.3198, 0.2846, 1.0000]])\n",
            "distiluse-base-multilingual-cased-v2\n",
            "tensor([[1.0000, 0.3283, 0.1343, 0.4413],\n",
            "        [0.3283, 1.0000, 0.2912, 0.3344],\n",
            "        [0.1343, 0.2912, 1.0000, 0.2602],\n",
            "        [0.4413, 0.3344, 0.2602, 1.0000]])\n",
            "paraphrase-multilingual-MiniLM-L12-v2\n",
            "tensor([[1.0000, 0.6386, 0.3326, 0.6615],\n",
            "        [0.6386, 1.0000, 0.3894, 0.5164],\n",
            "        [0.3326, 0.3894, 1.0000, 0.4497],\n",
            "        [0.6615, 0.5164, 0.4497, 1.0000]])\n",
            "paraphrase-multilingual-mpnet-base-v2\n",
            "tensor([[1.0000, 0.6006, 0.4287, 0.7956],\n",
            "        [0.6006, 1.0000, 0.3881, 0.5237],\n",
            "        [0.4287, 0.3881, 1.0000, 0.4295],\n",
            "        [0.7956, 0.5237, 0.4295, 1.0000]])\n"
          ]
        }
      ],
      "source": [
        "mutlilingual_models = [\"distiluse-base-multilingual-cased-v1\", \"distiluse-base-multilingual-cased-v2\",\n",
        "          \"paraphrase-multilingual-MiniLM-L12-v2\", \"paraphrase-multilingual-mpnet-base-v2\"]\n",
        "for model_title in mutlilingual_models:\n",
        "    model = SentenceTransformer(\"sentence-transformers/\"+model_title)\n",
        "\n",
        "    embeddings = model.encode(sentences)\n",
        "    similarities = model.similarity(embeddings, embeddings)\n",
        "    print(model_title)\n",
        "    print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We chose multilingual models since the text is in German. As you can see, we have four models which give four different results. Why is that? The explanation has to do with what they were trained for, and the tradeoff between speed and quality. \n",
        "\n",
        "[On their website](https://sbert.net/docs/sentence_transformer/pretrained_models.html#original-models) they show the performance of each model on a sentence embedding vs semantic search task, as wekk as their speed. The models that seem to agree with human judgement the most, namely \"paraphrase-multilingual-mpnet-base-v2\", is the largest one, with the best average performance, but also fairly fast all things considered. Which model works best for your usecase should be evaluated carefully, as you usually only compute the vector embeddings once."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GHhaFjSnNgge"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
