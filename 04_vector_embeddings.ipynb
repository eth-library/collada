{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Embeddings\n",
    "You might remember from the topic modeling notebook how I kept mentioning that these days, for most of what topic modeling used to be used for you would actually turn to vector embeddings. Here we are.\n",
    "\n",
    "Vector embeddigs are used in order to make textual or image information machine readable, but they have an additional usage: They encode some sort of semantic meaning. As John Rupert Firth once said \"You shall know a word by the company it keeps\" and word embeddings do exactly that. Moreover, since they are vectors, you can do vector operations with them in order to see what the training data (the company our word keeps) semantically say about the word itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuPYHiwzNggj"
   },
   "source": [
    "## Embedding space\n",
    "Here we embed E-Periodica articles in order to be able to find similar ones simply based on semantic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uqnCcJ85wMu"
   },
   "source": [
    "Here you create the articles dictionary based on .txt files. If you only have PDFs, check out the notebook on OCR which explains not only how to apply OCR on images but also how to extract embedded OCR text from PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwgJR9CW5vr0"
   },
   "outputs": [],
   "source": [
    "with open(\"data/embedding_data/grs-001_1921_13__298_d.txt\",\"r\") as fin:\n",
    "    input_text_1 = fin.readlines()\n",
    "    if len(input_text_1) == 1:\n",
    "        input_text_1 = input_text_1[0].split(\".\")\n",
    "\n",
    "with open(\"data/embedding_data/grs-001_1921_13__393_d.txt\",\"r\") as fin:\n",
    "    input_text_2 = fin.readlines()\n",
    "    if len(input_text_2) == 1:\n",
    "        input_text_2 = input_text_2[0].split(\".\")\n",
    "\n",
    "with open(\"data/embedding_data/grs-001_1922_14__563_d.txt\",\"r\") as fin:\n",
    "    input_text_4 = fin.readlines()\n",
    "    if len(input_text_4) == 1:\n",
    "        input_text_4 = input_text_4[0].split(\".\")\n",
    "\n",
    "with open(\"data/embedding_data/grs-001_1923_15__447_d.txt\",\"r\") as fin:\n",
    "    input_text_3 = fin.readlines()\n",
    "    if len(input_text_3) == 1:\n",
    "        input_text_3 = input_text_3[0].split(\".\")\n",
    "\n",
    "#create dictionary for each article\n",
    "article_dict = {\"article1\": {\"text\": input_text_1},\n",
    "                \"article2\": {\"text\": input_text_2},\n",
    "                \"article3\": {\"text\": input_text_3},\n",
    "                \"article4\": {\"text\": input_text_4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "This part is more for your understanding than anything else. BERT is not ideal for embedding large amounts of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrvrB4Od6RjA"
   },
   "source": [
    "Here we load the necessary libraries, tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JFkkAPHNggj"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "model = BertModel.from_pretrained('bert-base-german-cased',\n",
    "                                  output_hidden_states = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW_dpTvk6f92"
   },
   "source": [
    "This part checks if we have a GPU, and if we do it runs the computation on GPU. That makes the embedding much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuTHGLSm6fBB"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx4gvWpdNggj"
   },
   "source": [
    "We go through each article and embed it. To be exact, we embed each word and then average it out over a \n",
    "sentence. For four articles this takes 1 minute on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tC2jAh2QNggj"
   },
   "outputs": [],
   "source": [
    "for article_i in article_dict:\n",
    "    bert_emb = []\n",
    "    for sentence in article_dict[article_i][\"text\"]:\n",
    "        bert_tokens = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(bert_tokens[\"input_ids\"], bert_tokens[\"attention_mask\"])\n",
    "            state = outputs[\"last_hidden_state\"]\n",
    "            emb = (state*bert_tokens[\"attention_mask\"][:,:,None]).sum(dim=1) / bert_tokens[\"attention_mask\"][:,:,None].sum(dim=1)\n",
    "            bert_emb.append(emb)\n",
    "    article_dict[article_i][\"bert_embedding\"] = bert_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5-oE2KVNggk"
   },
   "source": [
    "We average everything each article says and create a matrix of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmQXDlC5Nggk"
   },
   "outputs": [],
   "source": [
    "V = []\n",
    "origin = np.array([[0]*768,[0]*768])\n",
    "for article_i in article_dict.keys():\n",
    "    mean = (sum(article_dict[article_i][\"bert_embedding\"]) / len(article_dict[article_i][\"bert_embedding\"]))\n",
    "    V.append(mean)\n",
    "V = torch.stack(V).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZw76sxDNggk"
   },
   "source": [
    "We create the distance matrix of each article to each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUe2x8tJNggk"
   },
   "outputs": [],
   "source": [
    "distance_matrix = metrics.pairwise_distances(V[:,0,:])\n",
    "\n",
    "#the distance of the article_i to itself is set to infinite\n",
    "for i in range(len(distance_matrix)):\n",
    "    distance_matrix[i][i] = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = distance_matrix.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distances are okay-ish but it doesn't really show how similar article 1 and article 4 are, it just looks like they're slightly more similar than article 4 and the other two articles. \n",
    "\n",
    "**Can we do better?** Yes. So far we've averaged the embeddings twice, once over each sentence and then even over each article! A lot of information gets lost there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentenceTransformer\n",
    "The [SentenceTransformer](https://sbert.net/) is a fine-tuned version of BERT, where the authors of the paper trained a so-called Siamese network in order to have a model specifically trained for similarity. Recall that BERT does similarity moreso out of the definition of a vector, rather than on purpose. \n",
    "\n",
    "Siamese networks train the vector embeddings using a positive and a negative example, and then try to jointly maximize the distance from our vector to the negative example, as well as minimize the distance from our vector to the positive example. Let's see how they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#https://sbert.net/docs/sentence_transformer/pretrained_models.html#semantic-similarity-models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \" \".join(article_dict[\"article1\"][\"text\"]),\n",
    "    \" \".join(article_dict[\"article2\"][\"text\"]),\n",
    "    \" \".join(article_dict[\"article3\"][\"text\"]),\n",
    "    \" \".join(article_dict[\"article4\"][\"text\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutlilingual_models = [\"distiluse-base-multilingual-cased-v1\", \"distiluse-base-multilingual-cased-v2\",\n",
    "                       \"paraphrase-multilingual-MiniLM-L12-v2\", \"paraphrase-multilingual-mpnet-base-v2\"]\n",
    "for model_title in mutlilingual_models:\n",
    "    model = SentenceTransformer(\"sentence-transformers/\"+model_title)\n",
    "\n",
    "    embeddings = model.encode(sentences)\n",
    "    similarities = model.similarity(embeddings, embeddings)\n",
    "    print(model_title)\n",
    "    print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose multilingual models since the text is in German. As you can see, we have four models which give four different results. Why is that? The explanation has to do with what they were trained for, and the trade-off between speed and quality. \n",
    "\n",
    "[On their website](https://sbert.net/docs/sentence_transformer/pretrained_models.html#original-models) they show the performance of each model on a sentence embedding vs semantic search task, as well as their speed. The models that seem to agree with human judgment the most, namely \"paraphrase-multilingual-mpnet-base-v2\", is the largest one, with the best average performance, but also fairly fast all things considered. Which model works best for your use-case should be evaluated carefully, as you usually only compute the vector embeddings once."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GHhaFjSnNgge"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
